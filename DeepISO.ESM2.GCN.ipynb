{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from Bio.PDB import *\n",
    "from Bio.PDB.DSSP import DSSP\n",
    "\n",
    "class ProteinDataProcessor:\n",
    "    def __init__(self, pesto_file_path, pdb_directory):\n",
    "        self.pesto_file_path = pesto_file_path\n",
    "        self.pdb_directory = pdb_directory\n",
    "        self.pesto_dict = {}\n",
    "        self.secondary_dict = {}\n",
    "        self.ss_mapping = {'H': 0, 'G': 0, 'I': 0, 'E': 1, 'T': 2, 'B': 2, 'S': 2, '-': 3}\n",
    "\n",
    "    def read_pesto_file(self):\n",
    "        with open(self.pesto_file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                columns = line.strip().split('\\t')\n",
    "                protein_name = columns[0]\n",
    "                data_values = [float(value) for value in columns[1:] if value != '0']  \n",
    "                self.pesto_dict[protein_name] = data_values\n",
    "\n",
    "    def process_secondary_structure(self):\n",
    "        for pdb_filename in os.listdir(self.pdb_directory):\n",
    "            pdb_filepath = os.path.join(self.pdb_directory, pdb_filename)\n",
    "            parser = PDBParser(QUIET=True)\n",
    "            structure = parser.get_structure(\"protein\", pdb_filepath)\n",
    "            model = structure[0]\n",
    "            dssp = DSSP(model, pdb_filepath)\n",
    "            protein_data = []\n",
    "\n",
    "            for residue in dssp:\n",
    "                res_id = residue[1]\n",
    "                ss = residue[2]\n",
    "                code = self.ss_mapping[ss]\n",
    "                codes = [code]  \n",
    "                #one-hot encoding\n",
    "                one_hot_codes = np.eye(4)[codes]  \n",
    "                protein_data.append(one_hot_codes)\n",
    "\n",
    "            protein_tensor = torch.tensor(protein_data, dtype=torch.float32).squeeze()\n",
    "            protein_name = os.path.splitext(pdb_filename)[0]\n",
    "            self.secondary_dict[protein_name] = protein_tensor\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.read_pesto_file()\n",
    "        self.process_secondary_structure()\n",
    "        return self.pesto_dict, self.secondary_dict\n",
    "\n",
    "file_path = \"/mnt/disk1/guoxiaokun/isoform/PeSTO/PeSTo-main/geometirc.allfeature.txt\"\n",
    "pdb_directory = \"/mnt/disk1/guoxiaokun/isoform/double.GCN/PDB\"\n",
    "\n",
    "processor = ProteinDataProcessor(file_path, pdb_directory)\n",
    "pesto_dict, secondary_dict = processor.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from tqdm import tqdm\n",
    "from Bio.PDB import PDBParser\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import scipy.spatial.distance as dist\n",
    "import biotite.structure.io.pdb as pdb\n",
    "\n",
    "def get_edge(coords, protein_name, threshold, pesto_dict):\n",
    "    \"\"\"Compute edge connections for a protein.\"\"\"\n",
    "    res_probs = pesto_dict[protein_name]\n",
    "    dists = dist.cdist(coords, coords)\n",
    "    edges = []\n",
    "    for i in range(len(dists)):\n",
    "        for j in range(len(dists)):\n",
    "            if res_probs[i] >= 0.1 and res_probs[j] >= 0.1 and i != j and dists[i, j] < threshold  and abs(i - j) != 1:\n",
    "                edges.append((i, j))\n",
    "            if i != j and dists[i, j] < threshold  and abs(i - j) != 1:\n",
    "                edges.append((i, j))\n",
    "    return edges\n",
    "\n",
    "# 生成蛋白质的图数据\n",
    "def generate_edge_index(esm_file, pdb_file, protein_name, threshold, pesto_dict):\n",
    "    esm_feature = torch.load(esm_file)['representations'][33]\n",
    "    protein_feature_esm = torch.load(esm_file)['mean_representations'][33]\n",
    "    \n",
    "    with open(pdb_file, 'r') as f:\n",
    "        model = pdb.PDBFile.read(f).get_structure(model=1)\n",
    "    x = esm_feature\n",
    "    esm2 = protein_feature_esm\n",
    "    coord = [i.coord for i in model if i.atom_name == \"CA\"]\n",
    "    edge_index = torch.tensor(get_edge(coord, protein_name, threshold, pesto_dict)).T\n",
    "    return Data(x=x, edge_index=edge_index, esm2=esm2)\n",
    "\n",
    "# Generate the graph and merge features\n",
    "def generate_protein_graphs(pdb_list_file, pdb_dir, esm_dir, threshold, pesto_dict, secondary_dict):\n",
    "    with open(pdb_list_file, 'r') as file:\n",
    "        f_names = [protein_name.rstrip() for protein_name in file]\n",
    "        pdb_files = [os.path.join(pdb_dir, protein_name + '.pdb') for protein_name in f_names]\n",
    "        esm_files = [os.path.join(esm_dir, protein_name + '.pt') for protein_name in f_names]\n",
    "    p = Pool(5)\n",
    "    result = [[protein_name, p.apply_async(generate_edge_index, (esm_file, pdb_file, protein_name, threshold, pesto_dict))] \n",
    "              for esm_file, pdb_file, protein_name in zip(esm_files, pdb_files, f_names)]\n",
    "    p.close()\n",
    "    p.join()\n",
    "    protein_dict = {k: v.get() for (k, v) in result}\n",
    "    for protein_name, data in protein_dict.items():\n",
    "        esm_fea = protein_dict[protein_name].x\n",
    "        protein_dict[protein_name].x = torch.cat((secondary_dict[protein_name], esm_fea), dim=1)\n",
    "    \n",
    "    return protein_dict\n",
    "\n",
    "pdb_list_file = '/mnt/disk1/guoxiaokun/isoform/double.GCN/pdb.list.txt'\n",
    "pdb_dir = '/mnt/disk1/guoxiaokun/isoform/double.GCN/PDB/'\n",
    "esm_dir = '/mnt/disk1/guoxiaokun/isoform/double.GCN/isoform_esm2/'\n",
    "threshold = 8\n",
    "protein_dict = generate_protein_graphs(pdb_list_file, pdb_dir, esm_dir, threshold, pesto_dict, secondary_dict)\n",
    "\n",
    "# The feature of protein\n",
    "for protein_name, data in protein_dict.items():\n",
    "    print(f\"{protein_name}: {data.x.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.sparse as spp\n",
    "\n",
    "class ProteinDataLoader:\n",
    "    def __init__(self, protein_dict, train_file, test_file, batch_size=3, seed=2066):\n",
    "        self.device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "        self.protein_dict = protein_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.setup_seed(self.seed)\n",
    "        \n",
    "        # Load train and test data\n",
    "        self.train_file = train_file\n",
    "        self.test_file = test_file\n",
    "        self.train_df = pd.read_csv(self.train_file, sep=\"\\t\", header=None)\n",
    "        self.test_df = pd.read_csv(self.test_file, sep=\"\\t\", header=None)\n",
    "        \n",
    "        # Prepare datasets and dataloaders\n",
    "        self.test_data = self.generate_data(self.test_df)\n",
    "        self.test_dataset = ProteinDataset(self.test_data)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "\n",
    "        self.raw_train_data = self.generate_data(self.train_df)\n",
    "        self.raw_train_dataset = ProteinDataset(self.raw_train_data)\n",
    "        self.raw_train_loader = DataLoader(self.raw_train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def setup_seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    def generate_data(self, df):\n",
    "        \"\"\"Generate data list from dataframe and protein dictionary.\"\"\"\n",
    "        data_list = []\n",
    "        for i in range(len(df)):\n",
    "            protein1, protein2, label = df.iloc[i]\n",
    "            data1 = self.protein_dict[protein1]\n",
    "            data2 = self.protein_dict[protein2]\n",
    "            data_list.append((data1, data2, torch.tensor([label], dtype=torch.float).to(self.device)))\n",
    "        return data_list\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Custom collate function for DataLoader.\"\"\"\n",
    "        data1 = [item[0] for item in batch]\n",
    "        data2 = [item[1] for item in batch]\n",
    "        label = torch.stack([item[2] for item in batch])\n",
    "        return [data1, data2, label]\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class for protein interaction data.\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def weights_init(m):\n",
    "     if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "       nn.init.kaiming_normal_(m.weight, mode='fan_in',\n",
    "                                 nonlinearity='leaky_relu')\n",
    "       \n",
    "train_file = 'train.txt'\n",
    "test_file = 'test.txt'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# Create an instance of ProteinDataLoader\n",
    "data_loader = ProteinDataLoader(protein_dict, train_file, test_file)\n",
    "\n",
    "test_loader = data_loader.test_loader\n",
    "raw_train_loader = data_loader.raw_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "# GCN模型定义\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 512).to(device)\n",
    "        self.conv2 = GCNConv(512, 256).to(device)\n",
    "        \n",
    "        self.cnn1 = torch.nn.Conv1d(1, 8, kernel_size=2, stride=1) \n",
    "        self.normal_layer3 = torch.nn.Linear(511, 511)\n",
    "\n",
    "        self.cnn2 = torch.nn.Conv1d(8, 16, kernel_size=2, stride=1) \n",
    "        self.normal_layer4 = torch.nn.Linear(510, 510)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(16*510, 2560).to(device)\n",
    "        self.fc2 = torch.nn.Linear(2560, 512).to(device)\n",
    "        self.fc3 = torch.nn.Linear(512, 128).to(device)\n",
    "        self.fc4 = torch.nn.Linear(128, 1).to(device)\n",
    "        self.fc5 = torch.nn.Linear(128, 1).to(device)\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.dropout = torch.nn.Dropout(0.5).to(device)  # Dropout layer 0.2\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        x1, edge_index1 = data1.x.to(device), data1.edge_index.to(device)\n",
    "        x2, edge_index2 = data2.x.to(device), data2.edge_index.to(device)\n",
    "\n",
    "        x1 = F.leaky_relu(self.conv1(x1, edge_index1))   \n",
    "        x1 = F.leaky_relu(self.conv2(x1, edge_index1))\n",
    "        x1 = torch.mean(x1, 0, keepdim=True)  # Average pooling\n",
    "\n",
    "        x2 = F.leaky_relu(self.conv1(x2, edge_index2))\n",
    "        x2 = F.leaky_relu(self.conv2(x2, edge_index2))\n",
    "        x2 = torch.mean(x2, 0, keepdim=True)  # Average pooling\n",
    "        out = torch.cat((x1,x2),dim=1)\n",
    "        \n",
    "        out = F.leaky_relu(self.cnn1(out))\n",
    "        out = self.normal_layer3(out)\n",
    "        out = F.leaky_relu(self.cnn2(out))\n",
    "        out = self.normal_layer4(out).view(-1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "# train and test\n",
    "def train_and_test(model_class, raw_train_loader, test_loader, device, num_epochs, lr, fold_count=5):\n",
    "    for kf in range(fold_count):\n",
    "        print(f\"kf: {kf}\\n\")\n",
    "        model = model_class(1284, 1).to(device)\n",
    "        model.apply(weights_init)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "        best_auc = 0.0  \n",
    "        best_model_path = f'best_model_fold{kf}.pth'  \n",
    "\n",
    "        for epoch in range(num_epochs): \n",
    "            start = time.time() \n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            y_trues, y_preds = [], []\n",
    "\n",
    "            # train dataset\n",
    "            for data1, data2, label in raw_train_loader:\n",
    "                data1 = [d1.to(device) for d1 in data1]\n",
    "                data2 = [d2.to(device) for d2 in data2]\n",
    "                out = torch.stack([model(data1[i], data2[i]) for i in range(len(data1))])\n",
    "                loss = criterion(out, label)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                y_trues.extend(label.to('cpu').tolist()) \n",
    "                y_preds.extend(out.to('cpu').detach().numpy().flatten())  \n",
    "                total_loss += loss.item()\n",
    "\n",
    "            train_loss = total_loss / len(raw_train_loader)\n",
    "\n",
    "            model.eval()\n",
    "            total_loss = 0\n",
    "            y_trues, y_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for data1, data2, label in test_loader:\n",
    "                    data1 = [d1.to(device) for d1 in data1]\n",
    "                    data2 = [d2.to(device) for d2 in data2]\n",
    "                    out = torch.stack([model(data1[i], data2[i]) for i in range(len(data1))])\n",
    "                    loss = criterion(out, label)\n",
    "                    \n",
    "                    y_trues.extend(label.to('cpu').tolist())  \n",
    "                    y_preds.extend(out.to('cpu').detach().numpy().flatten()) \n",
    "                    total_loss += loss.item()\n",
    "\n",
    "            val_loss = total_loss / len(test_loader)\n",
    "            auc = roc_auc_score(y_trues, y_preds)\n",
    "            print(f\"epoch: {epoch}, train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            # save best model\n",
    "            if auc > best_auc:\n",
    "                best_auc = auc\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"New best model found with AUC: {best_auc:.4f}. Model saved to {best_model_path}\")\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Best AUC for fold {kf}: {best_auc:.4f}\\n\")\n",
    "\n",
    "\n",
    "train_and_test(GCN, raw_train_loader, test_loader, device, num_epochs=50, lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.sparse as spp\n",
    "from zzd.utils.assess import multi_scores as scores\n",
    "class ProteinDataLoader:\n",
    "    def __init__(self, protein_dict, train_file, test_file, batch_size=3, seed=2066):\n",
    "        self.device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "        self.protein_dict = protein_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.setup_seed(self.seed)\n",
    "        \n",
    "        # Load train and test data\n",
    "        self.train_file = train_file\n",
    "        self.test_file = test_file\n",
    "        self.train_df = pd.read_csv(self.train_file, sep=\"\\t\", header=None)\n",
    "        self.test_df = pd.read_csv(self.test_file, sep=\"\\t\", header=None)\n",
    "        \n",
    "        # Prepare datasets and dataloaders no shuffle\n",
    "        self.test_data = self.generate_data(self.test_df)\n",
    "        self.test_dataset = ProteinDataset(self.test_data)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "\n",
    "        self.raw_train_data = self.generate_data(self.train_df)\n",
    "        self.raw_train_dataset = ProteinDataset(self.raw_train_data)\n",
    "        self.raw_train_loader = DataLoader(self.raw_train_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def setup_seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    def generate_data(self, df):\n",
    "        \"\"\"Generate data list from dataframe and protein dictionary.\"\"\"\n",
    "        data_list = []\n",
    "        for i in range(len(df)):\n",
    "            protein1, protein2, label = df.iloc[i]\n",
    "            data1 = self.protein_dict[protein1]\n",
    "            data2 = self.protein_dict[protein2]\n",
    "            data_list.append((data1, data2, torch.tensor([label], dtype=torch.float).to(self.device)))\n",
    "        return data_list\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Custom collate function for DataLoader.\"\"\"\n",
    "        data1 = [item[0] for item in batch]\n",
    "        data2 = [item[1] for item in batch]\n",
    "        label = torch.stack([item[2] for item in batch])\n",
    "        return [data1, data2, label]\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class for protein interaction data.\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "       \n",
    "train_file = 'train.txt'\n",
    "test_file = 'test.txt'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data_loader = ProteinDataLoader(protein_dict, train_file, test_file)\n",
    "# Access the dataloaders\n",
    "test_loader = data_loader.test_loader\n",
    "raw_train_loader = data_loader.raw_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP\tTN\tFP\tFN\tprecision\trecall\tspecificity\tAcc\tMCC\tf1\tAUROC\tAUPRC\n",
      "118\t117\t34\t31\t0.7763\t0.792\t0.775\t0.783\t0.567\t0.784\t0.836\t0.824\n",
      "Scores for test: (118, 117, 34, 31, 0.7763, 0.7919, 0.7748, 0.78333, 0.56682, 0.78405, 0.83644, 0.82429)\n",
      "TP\tTN\tFP\tFN\tprecision\trecall\tspecificity\tAcc\tMCC\tf1\tAUROC\tAUPRC\n",
      "564\t587\t12\t37\t0.9792\t0.938\t0.980\t0.959\t0.919\t0.958\t0.994\t0.995\n",
      "Scores for train: (564, 587, 12, 37, 0.9792, 0.9384, 0.98, 0.95917, 0.91914, 0.95837, 0.99409, 0.99458)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def setup_model_and_predict(model_class, model_path, device, test_loader, train_loader, test_file, train_file, output_test_file, output_train_file):\n",
    "    \"\"\"\n",
    "    Function to load a model, perform predictions on test and train data, and save the results to files.\n",
    "\n",
    "    Parameters:\n",
    "    - model_class: The model class (e.g., GCN).\n",
    "    - model_path: The path to the trained model file (.pth).\n",
    "    - device: The device to run the model on (e.g., 'cuda' or 'cpu').\n",
    "    - test_loader: DataLoader for test data.\n",
    "    - train_loader: DataLoader for train data.\n",
    "    - test_file: File containing test data information.\n",
    "    - train_file: File containing train data information.\n",
    "    - output_test_file: File path to save test prediction results.\n",
    "    - output_train_file: File path to save train prediction results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the model\n",
    "        model = model_class(1284, 1).to(device)  # 1284 is the input feature dimension, 1 is the output dimension\n",
    "        model.load_state_dict(torch.load(model_path)) \n",
    "        model.eval() \n",
    "        \n",
    "        def predict_and_save(loader, input_file, output_file, phase):\n",
    "            y_trues, y_preds = [], []\n",
    "            with torch.no_grad():\n",
    "                for data1, data2, label in loader:\n",
    "                    data1 = [d1.to(device) for d1 in data1]\n",
    "                    data2 = [d2.to(device) for d2 in data2]\n",
    "                    out = []\n",
    "                    for i in range(len(data1)):\n",
    "                        o = model(data1[i], data2[i])\n",
    "                        out.append(o)\n",
    "                    out = torch.stack(out)\n",
    "                    out = torch.sigmoid(out)\n",
    "                    y_preds.append(out.to('cpu').detach().tolist())\n",
    "                    y_trues.append(label.to('cpu').tolist())\n",
    "                \n",
    "            y_preds = [item for sublist in y_preds for item in sublist]\n",
    "            y_pred = np.array(y_preds)\n",
    "            pred_table = np.hstack((np.genfromtxt(input_file, str), y_pred.reshape(-1, 1)))\n",
    "            result_score = scores(pred_table[:, -2], pred_table[:, -1], show=True)\n",
    "            print(f\"Scores for {phase}: {result_score}\")\n",
    "\n",
    "            with open(output_file, \"w\") as f:\n",
    "                for i in range(len(pred_table)):\n",
    "                    f.write(f\"{pred_table[i, 0]}\\t{pred_table[i, 1]}\\t{pred_table[i, -2]}\\t{pred_table[i, -1]}\\n\")\n",
    "        \n",
    "        predict_and_save(test_loader, test_file, output_test_file, \"test\")\n",
    "        predict_and_save(train_loader, train_file, output_train_file, \"train\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred during model loading or prediction:\", str(e))\n",
    "\n",
    "\n",
    "setup_model_and_predict(GCN, \"/mnt/disk1/guoxiaokun/isoform/double.GCN/second.result.e25.p01.filteradj.fold2_best_model.pth\", device, test_loader, raw_train_loader, test_file, train_file, \"GCN.ESM2.test.predict.txt\", \"GCN.ESM2.train.predict.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
