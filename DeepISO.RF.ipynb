{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def ESM2_encoding(folder_path):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - folder_path: Path to the folder containing ESM2 encoded .pt files\n",
    "    \n",
    "    Returns:\n",
    "    - protein_dict: A dictionary where keys are protein names and values are their encodings\n",
    "    - num_proteins: The number of proteins\n",
    "    \"\"\"\n",
    "    protein_dict = {}\n",
    "    for file_name in sorted(os.listdir(folder_path)):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name.endswith(\".pt\"):\n",
    "            tensor = torch.load(file_path)['mean_representations'][33].tolist()  \n",
    "            protein_key = os.path.splitext(file_name)[0]\n",
    "            protein_dict[protein_key] = tensor\n",
    "\n",
    "    num_proteins = len(protein_dict)\n",
    "    print(\"Number of proteins:\", num_proteins)\n",
    "    return protein_dict, num_proteins\n",
    "\n",
    "protein_dict, _ = ESM2_encoding(\"/your/path/ESM2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def ProtT5_encoding(folder_path):\n",
    "    \"\"\"\n",
    "    Encodes protein sequences using ProtT5 embeddings by averaging the tensor values \n",
    "    and converting them into a flattened list.\n",
    "\n",
    "    Args:\n",
    "        folder_path: Path to the folder containing ProtT5-encoded .npy files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are protein names and values are the corresponding \n",
    "              flattened embedding tensors.\n",
    "    \"\"\"\n",
    "    prott5_dict = {}\n",
    "\n",
    "    for file_name in sorted(os.listdir(folder_path)):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name.endswith(\".npy\"):\n",
    "            tensor = np.mean(np.load(file_path), axis=0).reshape(1, -1).flatten().tolist()\n",
    "            protein_key = os.path.splitext(file_name)[0]\n",
    "            prott5_dict[protein_key] = tensor\n",
    "\n",
    "    return prott5_dict\n",
    "\n",
    "folder_path = \"/mnt/disk1/guoxiaokun/isoform/Prott5/encode\"\n",
    "prott5_dict = ProtT5_encoding(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('/mnt/disk1/guoxiaokun/isoform/PeSTO/PeSTo-main/RF/geometirc.sencond.simple.feature.txt', sep='\\t', header=None)\n",
    "\n",
    "data = data.iloc[:, 0]\n",
    "ran_data = pd.DataFrame(np.random.randn(756, 10))\n",
    "data = combined = pd.concat([data, ran_data], axis=1)\n",
    "n = data.shape[1]\n",
    "data.columns = list(range(n))\n",
    "\n",
    "protein_dict_geo = {}\n",
    "for i, row in data.iterrows():\n",
    "    key = row[0]\n",
    "    values = row[1:].tolist()\n",
    "    protein_dict_geo[key] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CT encoding\n",
    "import pandas as pd\n",
    "import itertools\n",
    "def CT(list_sequence):\n",
    "    t1 = ['1', '2', '3', '4', '5', '6', '7']\n",
    "    t2 = ['1', '2', '3', '4', '5', '6', '7']\n",
    "    t3 = ['1', '2', '3', '4', '5', '6', '7']\n",
    "    c = list(itertools.product(t1, t2, t3))\n",
    "    v343 = []\n",
    "    for i in c:\n",
    "        str = ''.join(i)\n",
    "        v343.append(str)\n",
    "    l15 = []\n",
    "    look = 0\n",
    "    \n",
    "    for sequence in list_sequence:\n",
    "        sequence = sequence.replace('A', '1')\n",
    "        sequence = sequence.replace('G', '1')\n",
    "        sequence = sequence.replace('V', '1')\n",
    "        sequence = sequence.replace('C', '2')\n",
    "        sequence = sequence.replace('D', '3')\n",
    "        sequence = sequence.replace('E', '3')\n",
    "        sequence = sequence.replace('F', '4')\n",
    "        sequence = sequence.replace('I', '4')\n",
    "        sequence = sequence.replace('L', '4')\n",
    "        sequence = sequence.replace('P', '4')\n",
    "        sequence = sequence.replace('H', '5')\n",
    "        sequence = sequence.replace('N', '5')\n",
    "        sequence = sequence.replace('Q', '5')\n",
    "        sequence = sequence.replace('W', '5')\n",
    "        sequence = sequence.replace('K', '6')\n",
    "        sequence = sequence.replace('R', '6')\n",
    "        sequence = sequence.replace('M', '7')\n",
    "        sequence = sequence.replace('S', '7')\n",
    "        sequence = sequence.replace('T', '7')\n",
    "        sequence = sequence.replace('Y', '7')\n",
    "        v = []\n",
    "        m = 0\n",
    "        while m < len(sequence) - 2:  \n",
    "            t = sequence[m:m + 3]\n",
    "            v.append(t)\n",
    "            m = m + 1\n",
    "        num = []\n",
    "        for i in v343:\n",
    "            n = v.count(i)\n",
    "            l = len(sequence)\n",
    "            l = l - 2\n",
    "            t = n / l\n",
    "            t = ('%.4f' % t)\n",
    "            t = float(t)\n",
    "            num.append(t)\n",
    "        l15.append(num)\n",
    "        look += 1\n",
    "    return l15\n",
    "\n",
    "file_path = \"/mnt/disk1/guoxiaokun/isoform/RF/data/isoform.tr.all.fa.csv\"\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "seq_list = b_list = data['seq'].tolist()\n",
    "CT_list = CT(seq_list)\n",
    "CT_df = pd.DataFrame(CT_list)\n",
    "CT_dict={}\n",
    "for i in range(795):\n",
    "    CT_dict[data.iloc[:,0][i]] = CT_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DPC\n",
    "def DPC(list_sequence):\n",
    "    t1 = ['A','G','V','C','D','E','F','I','L','P','H','N','Q','W','K','R','M','S','T','Y']\n",
    "    t2 = ['A','G','V','C','D','E','F','I','L','P','H','N','Q','W','K','R','M','S','T','Y']\n",
    "    c = list(itertools.product(t1, t2))\n",
    "    v400 = [] \n",
    "    for i in c:\n",
    "        str = ''.join(i)\n",
    "        v400.append(str)\n",
    "    look = 0\n",
    "    l15 = [] \n",
    "    for sequence in list_sequence:\n",
    "        v = []\n",
    "        m = 0\n",
    "        while m < len(sequence) - 1:  \n",
    "            t = sequence[m:m + 2]\n",
    "            v.append(t) \n",
    "            m = m + 1\n",
    "        num = [] \n",
    "        for i in v400:\n",
    "            n = v.count(i)\n",
    "            l = len(sequence)\n",
    "            l = l - 1\n",
    "            t = n / l\n",
    "            t = ('%.4f' % t)\n",
    "            t = float(t)\n",
    "            num.append(t)\n",
    "        l15.append(num)\n",
    "        look += 1\n",
    "    return l15\n",
    "\n",
    "file_path = \"/mnt/disk1/guoxiaokun/isoform/RF/data/isoform.tr.all.fa.csv\"\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "seq_list = b_list = data['seq'].tolist()\n",
    "DPC_list = DPC(seq_list)\n",
    "DPC_df = pd.DataFrame(DPC_list)\n",
    "DPC_dict={}\n",
    "for i in range(795):\n",
    "    DPC_dict[data.iloc[:,0][i]] = DPC_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AC\n",
    "H1 = [0.62,0.29,-0.9,-0.74,1.19,0.48,-0.4,1.38,-1.5,1.06,0.64,-0.78,0.12,-0.85,-2.53,-0.18,-0.05,1.08,0.81,0.26]\n",
    "H2 = [-0.5,-1,3,3,-2.5,0,-0.5,-1.8,3,-1.8,-1.3,2,0,0.2,3,0.3,-0.4,-1.5,-3.4,-2.3]\n",
    "V = [27.5,44.6,40,62,115.5,0,79,93.5,100,93.5,94.1,58.7,41.9,80.7,105,29.3,51.3,71.5,145.5,117.3]\n",
    "P1 = [8.1,5.5,13,12.3,5.2,9,10.4,5.2,11.3,4.9,5.7,11.6,8,10.5,10.5,9.2,8.6,5.9,5.4,6.2]\n",
    "P2 = [0.046,0.128,0.105,0.151,0.29,0,0.23,0.186,0.219,0.186,0.221,0.134,0.131,0.18,0.291,0.062,0.108,0.14,0.409,0.298]\n",
    "SASA = [1.181,1.461,1.587,1.862,2.228,0.881,2.025,1.81,2.258,1.931,2.034,1.655,1.468,1.932,2.56,1.298,1.525,1.645,2.663,2.368]\n",
    "NCI = [0.007187,-0.03661,-0.02382,0.006802,0.037552,0.179052,-0.01069,0.021631,0.017708,0.051672,0.002683,0.005392,0.239531,0.049211,0.043587,0.004627,0.003352,0.057004,0.037977,0.023599]\n",
    "dic1 = {\"H1\":H1,\"H2\":H2,\"V\":V,\"P1\":P1,\"P2\":P2,\"SASA\":SASA,\"NCI\":NCI} \n",
    "from pandas import DataFrame\n",
    "data1 = DataFrame(dic1) \n",
    "data1.rename({0:'A',1:'C',2:'D',3:'E',4:'F',5:'G',6:'H',7:'I',8:'K',9:'L',\n",
    "              10:'M',11:'N',12:'P',13:'Q',14:'R',15:'S',16:'T',17:'V',18:'W',19:'Y'},inplace=True) \n",
    "from numpy import * \n",
    "m1 = mean(H1)\n",
    "m2 = mean(H2)\n",
    "m3 = mean(V)\n",
    "m4 = mean(P1)\n",
    "m5 = mean(P2)\n",
    "m6 = mean(SASA)\n",
    "m7 = mean(NCI)\n",
    "mean = [m1,m2,m3,m4,m5,m6,m7] \n",
    "import numpy as np \n",
    "sd1 = np.std(H1,ddof=1)\n",
    "sd2 = np.std(H2,ddof=1)\n",
    "sd3 = np.std(V,ddof=1)\n",
    "sd4 = np.std(P1,ddof=1)\n",
    "sd5 = np.std(P2,ddof=1)\n",
    "sd6 = np.std(SASA,ddof=1)\n",
    "sd7 = np.std(NCI,ddof=1)\n",
    "sd = [sd1,sd2,sd3,sd4,sd5,sd6,sd7] \n",
    "data1.loc['mean']=mean \n",
    "data1.loc['sd']=sd\n",
    "all_aa = []\n",
    "for indexs in data1.index[:20]: \n",
    "    t = data1.loc[indexs].values[0:]\n",
    "    index = 0\n",
    "    aa = []\n",
    "    for i in t: \n",
    "        p = (i-mean[index])/sd[index] \n",
    "        index += 1\n",
    "        aa.append(p)\n",
    "    all_aa.append(aa)\n",
    "dic2 = {\"A\":all_aa[0],\"C\":all_aa[1],\"D\":all_aa[2],\"E\":all_aa[3],\"F\":all_aa[4],\n",
    "        \"G\":all_aa[5],\"H\":all_aa[6],\"I\":all_aa[7],\"K\":all_aa[8],\"L\":all_aa[9],\n",
    "        \"M\":all_aa[10],\"N\":all_aa[11],\"P\":all_aa[12],\"Q\":all_aa[13],\"R\":all_aa[14],\n",
    "        \"S\":all_aa[15],\"T\":all_aa[16],\"V\":all_aa[17],\"W\":all_aa[18],\"Y\":all_aa[19]}\n",
    "def AC(list_sequence):\n",
    "    look = 0\n",
    "    l15 = [] \n",
    "    for sequence in list_sequence:\n",
    "        L = len(sequence)\n",
    "        AC = []\n",
    "        for lag in range(1,30): \n",
    "            for j in range(0,7): \n",
    "                value1 = [] \n",
    "                for num in range(0,len(sequence)):\n",
    "                    aacid = sequence[num] \n",
    "                    phpro = dic2[aacid] \n",
    "                    vector = phpro[j]\n",
    "                    value1.append(vector)\n",
    "                R_k_j = sum(value1)\n",
    "                value2 = []\n",
    "                for num in range(0, (len(sequence)-lag-1)):\n",
    "                    aacid1 = sequence[num]  \n",
    "                    phpro1 = dic2[aacid1] \n",
    "                    R_i_j = phpro1[j] \n",
    "                    vector1 = R_i_j - ((1/L)*R_k_j)\n",
    "                    aacid2 = sequence[(num+lag)]  \n",
    "                    phpro2 = dic2[aacid2]  \n",
    "                    R_i_lag_j = phpro2[j]\n",
    "                    vector2 = R_i_lag_j - ((1/L)*R_k_j)\n",
    "                    vector3 = vector1*vector2\n",
    "                    value2.append(vector3)\n",
    "                v1 = sum(value2)\n",
    "                print(f\"len_seq:{len(sequence)},lag:{lag}\")\n",
    "                ac = (1/(len(sequence)-lag))*v1\n",
    "                AC.append(ac)\n",
    "        l15.append(AC)\n",
    "        look += 1\n",
    "    return l15\n",
    "\n",
    "file_path = \"/mnt/disk1/guoxiaokun/isoform/RF/data/isoform.tr.all.fa.csv\"\n",
    "data = pd.read_csv(file_path, sep='\\t')\n",
    "seq_list = b_list = data['seq'].tolist()\n",
    "print(seq_list)\n",
    "AC_list = AC(seq_list)\n",
    "AC_df = pd.DataFrame(AC_list)\n",
    "AC_dict={}\n",
    "for i in range(795):\n",
    "    AC_dict[data.iloc[:,0][i]] = AC_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "protein_seq = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(list(protein_seq))\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# 显示结果\n",
    "print('原始序列：', protein_seq)\n",
    "print('One-hot编码：\\n', onehot_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_data(train_file, test_file, protein_dict, encode=\"RF.ESM2\"):\n",
    "    \"\"\"\n",
    "    Process training and testing data, generating a vector file that contains Protein1, Protein2, and their labels.\n",
    "    \n",
    "    Parameters:\n",
    "        train_file (str): Path to the training data file.\n",
    "        test_file (str): Path to the testing data file.\n",
    "        protein_dict (dict): Dictionary containing protein names and their corresponding vectors.\n",
    "        encode (str): Encoding format and prefix for the output file name. Default is \"filter.ESM2\".\n",
    "    \n",
    "    Returns:\n",
    "        df_protein (DataFrame): Processed training DataFrame.\n",
    "        test_df_protein (DataFrame): Processed testing DataFrame.\n",
    "        merge_protein_dict (dict): Merged dictionary of protein vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the training and testing files\n",
    "    df = pd.read_csv(train_file, sep='\\t', header=None, names=[\"Protein1\", \"Protein2\", \"Label\"])\n",
    "    test_df = pd.read_csv(test_file, sep='\\t', header=None, names=[\"Protein1\", \"Protein2\", \"Label\"])\n",
    "    \n",
    "    protein_data = []\n",
    "    merge_protein_dict = {}\n",
    "\n",
    "    # Process training data\n",
    "    for index, row in df.iterrows():\n",
    "        protein1 = row[\"Protein1\"]\n",
    "        protein2 = row[\"Protein2\"]\n",
    "        label = row[\"Label\"]\n",
    "\n",
    "        if protein1 in protein_dict and protein2 in protein_dict:\n",
    "            list1 = protein_dict[protein1]\n",
    "            list2 = protein_dict[protein2]\n",
    "            merge_protein_dict[protein1] = list1\n",
    "            merge_protein_dict[protein2] = list2\n",
    "            protein_data.append((float(label), list1, list2))\n",
    "\n",
    "    df_protein = pd.DataFrame(protein_data, columns=[\"Label\", \"Protein1\", \"Protein2\"])\n",
    "    df_protein = pd.concat([df_protein[\"Label\"],\n",
    "                            df_protein[\"Protein1\"].apply(pd.Series),\n",
    "                            df_protein[\"Protein2\"].apply(pd.Series)], axis=1)\n",
    "    df_protein.to_csv(f\"{encode}.train.csv\", index=False, header=False)\n",
    "\n",
    "    # Process testing data\n",
    "    protein_data = []\n",
    "    for index, row in test_df.iterrows():\n",
    "        protein1 = row[\"Protein1\"]\n",
    "        protein2 = row[\"Protein2\"]\n",
    "        label = row[\"Label\"]\n",
    "\n",
    "        if protein1 in protein_dict and protein2 in protein_dict:\n",
    "            list1 = protein_dict[protein1]\n",
    "            list2 = protein_dict[protein2]\n",
    "            merge_protein_dict[protein1] = list1\n",
    "            merge_protein_dict[protein2] = list2\n",
    "            protein_data.append((float(label), list1, list2))\n",
    "\n",
    "    test_df_protein = pd.DataFrame(protein_data, columns=[\"Label\", \"Protein1\", \"Protein2\"])\n",
    "    test_df_protein = pd.concat([test_df_protein[\"Label\"],\n",
    "                                test_df_protein[\"Protein1\"].apply(pd.Series),\n",
    "                                test_df_protein[\"Protein2\"].apply(pd.Series)], axis=1)\n",
    "    test_df_protein.to_csv(f\"{encode}.test.csv\", index=False, header=False)\n",
    "\n",
    "    return df_protein, test_df_protein, merge_protein_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'train.txt'\n",
    "test_file = 'test.txt'\n",
    "df_protein, test_df_protein, merge_protein_dict = process_data(train_file, test_file, protein_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "def train_and_evaluate_random_forest(data, test_data, encode, save_model=True):\n",
    "    # Separate features and labels\n",
    "    X = data.iloc[:, 1:]\n",
    "    y = data.iloc[:, 0]\n",
    "    X_new_test = test_data.iloc[:, 1:]\n",
    "    y_new_test = test_data.iloc[:, 0]\n",
    "\n",
    "    #5-fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=22)\n",
    "    TPs, TNs, FPs, FNs = [], [], [], []\n",
    "    precisions, recalls, specificities, accuracies, MCCs, f1_scores, AUROCs, AUPRCs = [], [], [], [], [], [], [], []\n",
    "    fprs, tprs, aucs = [], [], []\n",
    "    precision_list, recall_list, pr_auc_list = [], [], []\n",
    "    y_pred_proba_new_list = []\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    best_n = 0\n",
    "    # Iterate through each fold of cross-validation\n",
    "    for n, (train_index, test_index) in enumerate(kf.split(X), start=1):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        rf = RandomForestClassifier()\n",
    "        params = {\n",
    "            'bootstrap': [False],\n",
    "            'ccp_alpha': [0.0],\n",
    "            'class_weight': [None],\n",
    "            'criterion': ['gini'],\n",
    "            'max_depth': [None],\n",
    "            'max_features': ['sqrt'],\n",
    "            'min_impurity_decrease': [0.0],\n",
    "            'min_samples_leaf': [1],\n",
    "            'min_samples_split': [2],\n",
    "            'min_weight_fraction_leaf': [0.0],\n",
    "            'n_estimators': [200],\n",
    "            'random_state': [None]\n",
    "        }\n",
    "        gs = GridSearchCV(rf, param_grid=params, cv=5, n_jobs=-1)\n",
    "        gs.fit(X_train, y_train)\n",
    "\n",
    "        rf_best = RandomForestClassifier(**gs.best_params_)\n",
    "        rf_best.fit(X_train, y_train)\n",
    "        # Predict on the test set\n",
    "        y_pred = rf_best.predict(X_test)\n",
    "        y_pred_proba = rf_best.predict_proba(X_test)[:, 1]\n",
    "        TP = np.sum((y_pred == 1) & (y_test == 1))\n",
    "        TN = np.sum((y_pred == 0) & (y_test == 0))\n",
    "        FP = np.sum((y_pred == 1) & (y_test == 0))\n",
    "        FN = np.sum((y_pred == 0) & (y_test == 1))\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        specificity = TN / (TN + FP)\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        MCC = (TP * TN - FP * FN) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "        y_pred_new = rf_best.predict(X_new_test)\n",
    "        y_pred_proba_new = rf_best.predict_proba(X_new_test)[:, 1]\n",
    "        y_pred_proba_new_list.append(y_pred_proba_new)\n",
    "        TP_new = np.sum((y_pred_new == 1) & (y_new_test == 1))\n",
    "        TN_new = np.sum((y_pred_new == 0) & (y_new_test == 0))\n",
    "        FP_new = np.sum((y_pred_new == 1) & (y_new_test == 0))\n",
    "        FN_new = np.sum((y_pred_new == 0) & (y_new_test == 1))\n",
    "        precision_new = TP_new / (TP_new + FP_new)\n",
    "        recall_new = TP_new / (TP_new + FN_new)\n",
    "        specificity_new = TN_new / (TN_new + FP_new)\n",
    "        accuracy_new = (TP_new + TN_new) / (TP_new + TN_new + FP_new + FN_new)\n",
    "        MCC_new = (TP_new * TN_new - FP_new * FN_new) / np.sqrt(\n",
    "            (TP_new + FP_new) * (TP_new + FN_new) * (TN_new + FP_new) * (TN_new + FN_new))\n",
    "        f1_score_new = 2 * (precision_new * recall_new) / (precision_new + recall_new)\n",
    "        fpr_new, tpr_new, _ = roc_curve(y_new_test, y_pred_proba_new)\n",
    "        roc_auc_new = auc(fpr_new, tpr_new)\n",
    "        precision_curve_new, recall_curve_new, _ = precision_recall_curve(y_new_test, y_pred_proba_new)\n",
    "        pr_auc_new = auc(recall_curve_new, precision_curve_new)\n",
    "        if best_score < roc_auc_new:\n",
    "            best_score = roc_auc_new\n",
    "            best_n = n\n",
    "            best_model = rf_best\n",
    "        # Save test predictions\n",
    "        independent_test = pd.DataFrame(y_pred_proba_new, columns=['predictions'])\n",
    "        merge_df = pd.concat([test_data, independent_test], axis=1)\n",
    "        # Record performance metrics\n",
    "        TPs.append(TP_new)\n",
    "        TNs.append(TN_new)\n",
    "        FPs.append(FP_new)\n",
    "        FNs.append(FN_new)\n",
    "        precisions.append(precision_new)\n",
    "        recalls.append(recall_new)\n",
    "        specificities.append(specificity_new)\n",
    "        accuracies.append(accuracy_new)\n",
    "        MCCs.append(MCC_new)\n",
    "        f1_scores.append(f1_score_new)\n",
    "        AUROCs.append(roc_auc_new)\n",
    "        AUPRCs.append(pr_auc_new)\n",
    "        precision_list.append(precision_curve_new)\n",
    "        recall_list.append(recall_curve_new)\n",
    "        pr_auc_list.append(pr_auc_new)\n",
    "        fprs.append(fpr_new)\n",
    "        tprs.append(tpr_new)\n",
    "        aucs.append(roc_auc_new)\n",
    "    col_means = [np.mean(col) for col in zip(*y_pred_proba_new_list)]\n",
    "    mean_independent_test = pd.DataFrame(col_means, columns=['predictions'])\n",
    "    merge_df = pd.concat([test_data, mean_independent_test], axis=1)\n",
    "    if save_model and best_model is not None:\n",
    "        joblib.dump(best_model, f'{encode}.rf.best_fold_{best_n}.pkl')\n",
    "\n",
    "    # Print the performance summary for each fold\n",
    "    print_performance_summary(TPs, TNs, FPs, FNs, precisions, recalls, specificities, accuracies, MCCs, f1_scores, AUROCs, AUPRCs)\n",
    "    # Plot PR curve and ROC curve\n",
    "    plot_precision_recall_curve(precision_list, recall_list, pr_auc_list)\n",
    "    plot_roc_curve(fprs, tprs, aucs)\n",
    "\n",
    "def print_performance_summary(TPs, TNs, FPs, FNs, precisions, recalls, specificities, accuracies, MCCs, f1_scores, AUROCs, AUPRCs):\n",
    "    metrics = {\n",
    "        'TP': TPs, 'TN': TNs, 'FP': FPs, 'FN': FNs,\n",
    "        'Precision': precisions, 'Recall': recalls, 'Specificity': specificities,\n",
    "        'Accuracy': accuracies, 'MCC': MCCs, 'F1 score': f1_scores,\n",
    "        'AUROC': AUROCs, 'AUPRC': AUPRCs\n",
    "    }\n",
    "    for name, values in metrics.items():\n",
    "        print(f\"{name}: mean = {np.mean(values)}, std = {np.std(values)}\")\n",
    "\n",
    "def plot_precision_recall_curve(precision_list, recall_list, pr_auc_list):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for precision_curve, recall_curve, pr_auc in zip(precision_list, recall_list, pr_auc_list):\n",
    "        plt.plot(recall_curve, precision_curve, lw=2, label=f'Fold (AUC = {pr_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(fprs, tprs, aucs):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for fpr, tpr, auc in zip(fprs, tprs, aucs):\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'Fold (AUC = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_random_forest(df_protein, test_df_protein, encode=\"RF.ESM2\", save_model=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "import joblib\n",
    "import numpy as np\n",
    "from zzd.utils.assess import multi_scores as scores\n",
    "\n",
    "def load_and_predict(model_path, train_data, test_data, train_file, test_file, output_train, output_test):\n",
    "    \"\"\"\n",
    "    Load the model and make predictions, saving the results to specified files.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_path: Path to the model file\n",
    "    - train_data: DataFrame for training data\n",
    "    - test_data: DataFrame for test data\n",
    "    - train_file: Path to the training set file for generating the prediction table\n",
    "    - test_file: Path to the test set file for generating the prediction table\n",
    "    - output_train: Output filename for training set predictions\n",
    "    - output_test: Output filename for test set predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(model_path, 'rb') as model_file:\n",
    "            loaded_model = joblib.load(model_file)\n",
    "    \n",
    "        model_params = loaded_model.get_params()\n",
    "        print(\"Model hyperparameters:\", model_params)\n",
    "        X_train = train_data.iloc[:, 1:]\n",
    "        y_train = train_data.iloc[:, 0]\n",
    "        X_test = test_data.iloc[:, 1:]\n",
    "        y_test = test_data.iloc[:, 0]\n",
    "\n",
    "        y_train_pred = loaded_model.predict_proba(X_train)[:, 1]\n",
    "        y_test_pred = loaded_model.predict_proba(X_test)[:, 1]\n",
    "        pred_table_train = np.hstack((np.genfromtxt(train_file, str), y_train_pred.reshape(-1, 1)))\n",
    "        pred_table_test = np.hstack((np.genfromtxt(test_file, str), y_test_pred.reshape(-1, 1)))\n",
    "        print(\"Training set evaluation results:\")\n",
    "        result_train = scores(pred_table_train[:, -2], pred_table_train[:, -1], show=True)\n",
    "        print(\"Test set evaluation results:\")\n",
    "        result_test = scores(pred_table_test[:, -2], pred_table_test[:, -1], show=True)\n",
    "\n",
    "        # Save prediction results\n",
    "        with open(output_train, 'w') as output_file_train:\n",
    "            for row in pred_table_train:\n",
    "                output_file_train.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "        with open(output_test, 'w') as output_file_test:\n",
    "            for row in pred_table_test:\n",
    "                output_file_test.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "        print(f\"Prediction results have been saved to {output_train} and {output_test}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred during model loading or prediction:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyperparameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "Training set evaluation results:\n",
      "TP\tTN\tFP\tFN\tprecision\trecall\tspecificity\tAcc\tMCC\tf1\tAUROC\tAUPRC\n",
      "548\t543\t56\t53\t0.9073\t0.912\t0.906\t0.909\t0.818\t0.910\t0.970\t0.969\n",
      "Test set evaluation results:\n",
      "TP\tTN\tFP\tFN\tprecision\trecall\tspecificity\tAcc\tMCC\tf1\tAUROC\tAUPRC\n",
      "105\t105\t46\t44\t0.6954\t0.705\t0.695\t0.700\t0.400\t0.700\t0.774\t0.786\n",
      "Prediction results have been saved to DeepISO.RF.prediction.ESM2.train.txt and DeepISO.RF.prediction.ESM2.test.txt\n"
     ]
    }
   ],
   "source": [
    "train_data = df_protein\n",
    "test_data = test_df_protein\n",
    "model_path = 'DeepISO.RF.pkl'\n",
    "train_file = 'train.txt'  \n",
    "test_file = 'test.txt'   \n",
    "\n",
    "load_and_predict(\n",
    "    model_path=model_path,\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    train_file=train_file,\n",
    "    test_file=test_file,\n",
    "    output_train='DeepISO.RF.prediction.ESM2.train.txt',\n",
    "    output_test='DeepISO.RF.prediction.ESM2.test.txt'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
