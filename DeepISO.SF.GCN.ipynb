{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class ProteinDataSFProcessor:\n",
    "    def __init__(self, foldseek_file, pesto_pickle_file):\n",
    "        self.foldseek_file = foldseek_file\n",
    "        self.pesto_pickle_file = pesto_pickle_file\n",
    "        \n",
    "        self.foldseek = {}\n",
    "        self.pesto_protein_dict = {}\n",
    "        self.element_mapping = {'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, \n",
    "                                'H': 6, 'I': 7, 'K': 8, 'L': 9, 'M': 10, 'N': 11, \n",
    "                                'P': 12, 'Q': 13, 'R': 14, 'S': 15, 'T': 16, \n",
    "                                'V': 17, 'W': 18, 'Y': 19, 'X': 20}\n",
    "    \n",
    "    def load_foldseek_encodings(self):\n",
    "        \"\"\"Loads FoldSeek encodings\"\"\"\n",
    "        with open(self.foldseek_file, 'r') as file:\n",
    "            for line in file:\n",
    "                name, seq = line.strip().split('\\t', 1)\n",
    "                encoding = []\n",
    "                for s in seq:\n",
    "                    vector = np.zeros(21)\n",
    "                    vector[self.element_mapping[s]] = 1\n",
    "                    encoding.append(vector)\n",
    "                encoding_tensor = torch.tensor(encoding, dtype=torch.float32)\n",
    "                self.foldseek[name] = encoding_tensor\n",
    "    \n",
    "    def load_pesto_pickle(self):\n",
    "        \"\"\"Loads PeSTO protein data from the pickle file.\"\"\"\n",
    "        with open(self.pesto_pickle_file, 'rb') as file:\n",
    "            self.pesto_protein_dict = pickle.load(file)\n",
    "    \n",
    "    def process_data(self):\n",
    "        \"\"\"Executes the full data loading process.\"\"\"\n",
    "        self.load_foldseek_encodings()\n",
    "        self.load_pesto_pickle()\n",
    "        print(\"Data loading complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path of feature file\n",
    "foldseek_file = \"./feature/GCN.SF.foldseek.feature.txt\"\n",
    "pesto_pickle_file = \"./feature/GCN.SF.PeSTO.feature.pkl\"\n",
    "\n",
    "# Initialize and process the data\n",
    "processor = ProteinDataSFProcessor(foldseek_file, pesto_pickle_file)\n",
    "processor.process_data()\n",
    "foldseek = processor.foldseek\n",
    "pesto_protein_dict = processor.pesto_protein_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from tqdm import tqdm\n",
    "from Bio.PDB import PDBParser\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import scipy.spatial.distance as dist\n",
    "import biotite.structure.io.pdb as pdb\n",
    "\n",
    "def get_edge(coords,protein_name, threshold=8):\n",
    "    \"\"\"Compute edge connections for a protein.\"\"\"\n",
    "    # Calculate distance matrix \n",
    "    dists = dist.cdist(coords, coords)\n",
    "    edges = []\n",
    "    filter_edges = []\n",
    "    for i in range(len(dists)):\n",
    "        for j in range(len(dists)):\n",
    "            if i != j and dists[i,j] < threshold and abs(i-j) != 1:\n",
    "                edges.append((i, j))\n",
    "           \n",
    "    return edges\n",
    "\n",
    "def generate_edge_index(esm_file,pdb_file,protein_name,threshold=8):\n",
    "    protein_name = protein_name       \n",
    "    with open(pdb_file,'r') as f:\n",
    "        model = pdb.PDBFile.read(f).get_structure(model=1)\n",
    "    \n",
    "    x = torch.tensor(pesto_protein_dict[protein_name], dtype=torch.float32)\n",
    "    esm2 = pesto_protein_dict[protein_name]\n",
    "    coord = [i.coord for i in model if i.atom_name==\"CA\"]\n",
    "    edge_index = torch.tensor(get_edge(coord,protein_name,threshold=8)).T\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index ,esm2=esm2 )\n",
    "\n",
    "# Generate the graph and merge features\n",
    "def generate_protein_graphs(pdb_list_file, pdb_dir, esm_dir, threshold):\n",
    "    with open(pdb_list_file,'r') as file:\n",
    "        f_names = [protein_name.rstrip() for protein_name in file]\n",
    "        pdb_files = [os.path.join(pdb_dir, protein_name + '.pdb') for protein_name in f_names]\n",
    "        esm_files = [os.path.join(esm_dir,protein_name + '.pt') for protein_name in f_names]\n",
    "        p = Pool(5)\n",
    "        result = [[protein_name, p.apply_async(generate_edge_index,(esm_file,pdb_file,protein_name,8))] for esm_file,pdb_file,protein_name in zip(esm_files,pdb_files,f_names)]  \n",
    "        p.close()\n",
    "        p.join()\n",
    "        protein_dict = {k:v.get() for (k,v) in result} \n",
    "        for protein_name,data in protein_dict.items():\n",
    "            esm_fea = protein_dict[protein_name]['x']\n",
    "            protein_dict[protein_name]['x'] = torch.cat((foldseek[protein_name],protein_dict[protein_name]['x']), dim=1)\n",
    "    \n",
    "    return protein_dict\n",
    "\n",
    "pdb_list_file = './pdb.list.txt'\n",
    "pdb_dir = '/your/path/PDB/'\n",
    "esm_dir = '/your/path/isoform_esm2/'\n",
    "threshold = 8\n",
    "protein_dict = generate_protein_graphs(pdb_list_file, pdb_dir, esm_dir, threshold)\n",
    "\n",
    "# The feature of protein\n",
    "for protein_name, data in protein_dict.items():\n",
    "    print(f\"{protein_name}: {data.x.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.sparse as spp\n",
    "\n",
    "class ProteinDataLoader:\n",
    "    def __init__(self, protein_dict, train_file, test_file, batch_size=3, seed=2066):\n",
    "        self.device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "        self.protein_dict = protein_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.setup_seed(self.seed)\n",
    "        \n",
    "        self.train_file = train_file\n",
    "        self.test_file = test_file\n",
    "        self.train_df = pd.read_csv(self.train_file, sep=\"\\t\", header=None)\n",
    "        self.test_df = pd.read_csv(self.test_file, sep=\"\\t\", header=None)\n",
    "        \n",
    "        self.test_data = self.generate_data(self.test_df)\n",
    "        self.test_dataset = ProteinDataset(self.test_data)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "\n",
    "        self.raw_train_data = self.generate_data(self.train_df)\n",
    "        self.raw_train_dataset = ProteinDataset(self.raw_train_data)\n",
    "        self.train_loader = DataLoader(self.raw_train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def setup_seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    def generate_data(self, df):\n",
    "        \"\"\"Generate data list from dataframe and protein dictionary.\"\"\"\n",
    "        data_list = []\n",
    "        for i in range(len(df)):\n",
    "            protein1, protein2, label = df.iloc[i]\n",
    "            data1 = self.protein_dict[protein1]\n",
    "            data2 = self.protein_dict[protein2]\n",
    "            data_list.append((data1, data2, torch.tensor([label], dtype=torch.float).to(self.device)))\n",
    "        return data_list\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        data1 = [item[0] for item in batch]\n",
    "        data2 = [item[1] for item in batch]\n",
    "        label = torch.stack([item[2] for item in batch])\n",
    "        return [data1, data2, label]\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def weights_init(m):\n",
    "     if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "       nn.init.kaiming_normal_(m.weight, mode='fan_in',\n",
    "                                 nonlinearity='leaky_relu')\n",
    "       \n",
    "train_file = 'train.txt'\n",
    "test_file = 'test.txt'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data_loader = ProteinDataLoader(protein_dict, train_file, test_file)\n",
    "test_loader = data_loader.test_loader\n",
    "raw_train_loader = data_loader.train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 512).to(device)\n",
    "        self.conv2 = GCNConv(512, 256).to(device)\n",
    "        \n",
    "        self.cnn1 = torch.nn.Conv1d(1, 8, kernel_size=2, stride=1) # (16,510)\n",
    "        self.normal_layer3 = torch.nn.Linear(511, 511)\n",
    "\n",
    "        self.cnn2 = torch.nn.Conv1d(8, 16, kernel_size=2, stride=1) #(16,254)\n",
    "        self.normal_layer4 = torch.nn.Linear(510, 510)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(16*510, 2560).to(device)\n",
    "        self.fc2 = torch.nn.Linear(2560, 512).to(device)\n",
    "        self.fc3 = torch.nn.Linear(512, 128).to(device)\n",
    "        self.fc4 = torch.nn.Linear(128, 1).to(device)\n",
    "        self.fc5 = torch.nn.Linear(128, 1).to(device)\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.dropout = torch.nn.Dropout(0.5).to(device) \n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        x1, edge_index1 = data1.x.to(device), data1.edge_index.to(device)\n",
    "        x2, edge_index2 = data2.x.to(device), data2.edge_index.to(device)\n",
    "\n",
    "        x1 = F.leaky_relu(self.conv1(x1, edge_index1))   \n",
    "        x1 = F.leaky_relu(self.conv2(x1, edge_index1))\n",
    "        x1 = torch.mean(x1, 0, keepdim=True)  # Average pooling of the features of all nodes\n",
    "\n",
    "        x2 = F.leaky_relu(self.conv1(x2, edge_index2))\n",
    "        x2 = F.leaky_relu(self.conv2(x2, edge_index2))\n",
    "        x2 = torch.mean(x2, 0, keepdim=True)  # Average pooling of the features of all nodes\n",
    "        out = torch.cat((x1,x2),dim=1)\n",
    "        \n",
    "        out = F.leaky_relu(self.cnn1(out))\n",
    "        out = self.normal_layer3(out)\n",
    "        out = F.leaky_relu(self.cnn2(out))\n",
    "        out = self.normal_layer4(out).view(-1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from zzd.utils.assess import multi_scores as scores\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "def train_and_evaluate_model(raw_train_loader, test_loader, model_class, device, num_epochs,lr):\n",
    "    for kf in range(5):\n",
    "        print(f\"kf:{kf}\")\n",
    "        best_auc = 0\n",
    "        best_model_path = f'DeepISO.SF.GCN.pth'\n",
    "        model = model_class(27, 1).to(device)\n",
    "        model.apply(weights_init)\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "        # early_stopping = EarlyStopping(patience=20, verbose=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            y_trues, y_preds = [], []\n",
    "            for data1, data2, label in raw_train_loader:\n",
    "                data1 = [d1.to(device) for d1 in data1]\n",
    "                data2 = [d2.to(device) for d2 in data2]\n",
    "                out = [model(data1[i], data2[i]) for i in range(len(data1))]\n",
    "                out = torch.stack(out)\n",
    "                loss = criterion(out, label)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                y_trues.append(label.cpu().tolist()[0])\n",
    "                y_preds.append(out.cpu().detach().tolist()[0])\n",
    "                total_loss += loss.item()\n",
    "            epoch_loss = total_loss / len(raw_train_loader)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_losses, val_y_trues, val_y_preds = [], [], []\n",
    "                for data1, data2, label in test_loader:\n",
    "                    data1 = [d1.to(device) for d1 in data1]\n",
    "                    data2 = [d2.to(device) for d2 in data2]\n",
    "                    out = [model(data1[i], data2[i]) for i in range(len(data1))]\n",
    "                    out = torch.stack(out)\n",
    "                    out = torch.sigmoid(out)\n",
    "                    loss = criterion(out, label)\n",
    "                    val_y_trues.extend(label.cpu().tolist())\n",
    "                    val_y_preds.extend(out.cpu().detach().tolist())\n",
    "                    val_losses.append(loss.item())\n",
    "                valid_loss = np.average(val_losses)\n",
    "                scheduler.step(valid_loss)\n",
    "\n",
    "                auc = roc_auc_score(val_y_trues, val_y_preds)\n",
    "                print(f\"epoch:{epoch}, train_loss:{epoch_loss:.4f}, val_loss:{valid_loss:.4f}, auc:{auc:.4f}, time:{time.time() - start_time:.1f}s\")\n",
    "\n",
    "                # Early stopping\n",
    "                # early_stopping(valid_loss)\n",
    "                # if early_stopping.early_stop:\n",
    "                #     print(\"Early stopping\")\n",
    "                #     break\n",
    "\n",
    "                if auc > best_auc:\n",
    "                    best_auc = auc\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "                    print(f\"Saved model with AUC: {best_auc:.4f}\")\n",
    "\n",
    "            # Evaluation on test set\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_y_trues, test_y_preds = [], []\n",
    "                for data1, data2, label in test_loader:\n",
    "                    data1 = [d1.to(device) for d1 in data1]\n",
    "                    data2 = [d2.to(device) for d2 in data2]\n",
    "                    out = [model(data1[i], data2[i]) for i in range(len(data1))]\n",
    "                    out = torch.stack(out)\n",
    "                    out = torch.sigmoid(out)\n",
    "                    test_y_preds.append(out.cpu().detach().tolist())\n",
    "                    test_y_trues.append(label.cpu().tolist())\n",
    "            # Save test results\n",
    "            pred_table = np.hstack((np.genfromtxt(test_file, str), np.array(test_y_preds).reshape(-1, 1)))\n",
    "            scores(pred_table[:, -2], pred_table[:, -1], show=True)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "train_and_evaluate_model(raw_train_loader, test_loader, GCN, device, num_epochs=100, lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.sparse as spp\n",
    "\n",
    "class ProteinDataLoader:\n",
    "    def __init__(self, protein_dict, train_file, test_file, batch_size=3, seed=2066):\n",
    "        self.device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "        self.protein_dict = protein_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.setup_seed(self.seed)\n",
    "        \n",
    "        self.train_file = train_file\n",
    "        self.test_file = test_file\n",
    "        self.train_df = pd.read_csv(self.train_file, sep=\"\\t\", header=None)\n",
    "        self.test_df = pd.read_csv(self.test_file, sep=\"\\t\", header=None)\n",
    "        \n",
    "        self.test_data = self.generate_data(self.test_df)\n",
    "        self.test_dataset = ProteinDataset(self.test_data)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "\n",
    "        self.train_data = self.generate_data(self.train_df)\n",
    "        self.train_dataset = ProteinDataset(self.train_data)\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def setup_seed(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    def generate_data(self, df):\n",
    "        \"\"\"Generate data list from dataframe and protein dictionary.\"\"\"\n",
    "        data_list = []\n",
    "        for i in range(len(df)):\n",
    "            protein1, protein2, label = df.iloc[i]\n",
    "            data1 = self.protein_dict[protein1]\n",
    "            data2 = self.protein_dict[protein2]\n",
    "            data_list.append((data1, data2, torch.tensor([label], dtype=torch.float).to(self.device)))\n",
    "        return data_list\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        data1 = [item[0] for item in batch]\n",
    "        data2 = [item[1] for item in batch]\n",
    "        label = torch.stack([item[2] for item in batch])\n",
    "        return [data1, data2, label]\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def weights_init(m):\n",
    "     if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "       nn.init.kaiming_normal_(m.weight, mode='fan_in',\n",
    "                                 nonlinearity='leaky_relu')\n",
    "       \n",
    "train_file = 'train.txt'\n",
    "test_file = 'test.txt'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data_loader = ProteinDataLoader(protein_dict, train_file, test_file)\n",
    "test_loader = data_loader.test_loader\n",
    "train_loader = data_loader.train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zzd.utils.assess import multi_scores as scores\n",
    "def load_model(model_path, input_dim, output_dim, device):\n",
    "    model = GCN(input_dim, output_dim).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    y_tures = []\n",
    "    y_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data1, data2, label in data_loader:\n",
    "            data1 = [d1.to(device) for d1 in data1]\n",
    "            data2 = [d2.to(device) for d2 in data2]\n",
    "            out = []\n",
    "            for i in range(len(data1)):\n",
    "                o = model(data1[i], data2[i])\n",
    "                out.append(o)\n",
    "            out = torch.stack(out)\n",
    "            out = torch.sigmoid(out)\n",
    "            y_preds.append(out.to('cpu').detach().tolist())\n",
    "            y_tures.append(label.to('cpu').tolist())\n",
    "    \n",
    "    y_preds = [item for sublist in y_preds for item in sublist]\n",
    "    y_pred = np.array(y_preds)\n",
    "    \n",
    "    return y_tures, y_pred\n",
    "\n",
    "def save_predictions_to_file(pred_table, output_file):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for i in range(len(pred_table)):\n",
    "            f.write(f\"{pred_table[i, 0]}\\t{pred_table[i, 1]}\\t{pred_table[i, -2]}\\t{pred_table[i, -1]}\\n\")\n",
    "\n",
    "def run_evaluation(model_path, test_file, train_file, test_loader, train_loader, device):\n",
    "    try:\n",
    "        model = load_model(model_path, 27, 1, device)\n",
    "        # test dataset\n",
    "        y_trues, y_preds = evaluate_model(model, test_loader, device)\n",
    "        pred_table = np.hstack((np.genfromtxt(test_file, str), y_preds.reshape(-1, 1)))\n",
    "        result_score = scores(pred_table[:, -2], pred_table[:, -1], show=True)  # 评估预测结果\n",
    "        save_predictions_to_file(pred_table, \"DeepISO.SFGCN.test.txt\")\n",
    "        # train dataset\n",
    "        y_trues, y_preds = evaluate_model(model, train_loader, device)\n",
    "        pred_table = np.hstack((np.genfromtxt(train_file, str), y_preds.reshape(-1, 1)))\n",
    "        result_score = scores(pred_table[:, -2], pred_table[:, -1], show=True)  # 评估预测结果\n",
    "        save_predictions_to_file(pred_table, \"DeepISO.SFGCN.train.txt\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"There is an error when loading or predicting with the model：\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation('DeepISO.SF.GCN.pth', \n",
    "               test_file='test.txt', \n",
    "               train_file='train.txt', \n",
    "               test_loader=test_loader, \n",
    "               train_loader=train_loader, \n",
    "               device='cuda:1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zzd.utils.assess import multi_scores as scores\n",
    "def load_model(model_path, device):\n",
    "    model = torch.load(model_path).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    y_tures = []\n",
    "    y_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data1, data2, label in data_loader:\n",
    "            data1 = [d1.to(device) for d1 in data1]\n",
    "            data2 = [d2.to(device) for d2 in data2]\n",
    "            out = []\n",
    "            for i in range(len(data1)):\n",
    "                o = model(data1[i], data2[i])\n",
    "                out.append(o)\n",
    "            out = torch.stack(out)\n",
    "            out = torch.sigmoid(out)\n",
    "            y_preds.append(out.to('cpu').detach().tolist())\n",
    "            y_tures.append(label.to('cpu').tolist())\n",
    "    \n",
    "    y_preds = [item for sublist in y_preds for item in sublist]\n",
    "    y_pred = np.array(y_preds)\n",
    "    \n",
    "    return y_tures, y_pred\n",
    "\n",
    "def save_predictions_to_file(pred_table, output_file):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for i in range(len(pred_table)):\n",
    "            f.write(f\"{pred_table[i, 0]}\\t{pred_table[i, 1]}\\t{pred_table[i, -2]}\\t{pred_table[i, -1]}\\n\")\n",
    "\n",
    "def run_evaluation(model_path, test_file, train_file, test_loader, train_loader, device):\n",
    "    try:\n",
    "        model = load_model(model_path, device)\n",
    "        # test dataset\n",
    "        y_trues, y_preds = evaluate_model(model, test_loader, device)\n",
    "        pred_table = np.hstack((np.genfromtxt(test_file, str), y_preds.reshape(-1, 1)))\n",
    "        result_score = scores(pred_table[:, -2], pred_table[:, -1], show=True)  # 评估预测结果\n",
    "        save_predictions_to_file(pred_table, \"DeepISO.SFGCN.test.txt\")\n",
    "        # train dataset\n",
    "        y_trues, y_preds = evaluate_model(model, train_loader, device)\n",
    "        pred_table = np.hstack((np.genfromtxt(train_file, str), y_preds.reshape(-1, 1)))\n",
    "        result_score = scores(pred_table[:, -2], pred_table[:, -1], show=True)  # 评估预测结果\n",
    "        save_predictions_to_file(pred_table, \"DeepISO.SFGCN.train.txt\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"There is an error when loading or predicting with the model：\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation('DeepISO.SF.GCN.pt', \n",
    "               test_file='test.txt', \n",
    "               train_file='train.txt', \n",
    "               test_loader=test_loader, \n",
    "               train_loader=train_loader, \n",
    "               device='cuda:1')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
