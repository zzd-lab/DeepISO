{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df_protein = pd.read_csv('merge.train.csv',header=None).iloc[:,2:]\n",
    "test_df_protein = pd.read_csv('merge.test.csv',header= None).iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "def train_and_evaluate_lr(data, test_data, encode, save_model=True):\n",
    "    # Separate features and labels\n",
    "    X = data.iloc[:, 1:]\n",
    "    y = data.iloc[:, 0]\n",
    "    X_new_test = test_data.iloc[:, 1:]\n",
    "    y_new_test = test_data.iloc[:, 0]\n",
    "\n",
    "    #5-fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=22)\n",
    "    TPs, TNs, FPs, FNs = [], [], [], []\n",
    "    precisions, recalls, specificities, accuracies, MCCs, f1_scores, AUROCs, AUPRCs = [], [], [], [], [], [], [], []\n",
    "    fprs, tprs, aucs = [], [], []\n",
    "    precision_list, recall_list, pr_auc_list = [], [], []\n",
    "    y_pred_proba_new_list = []\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    best_n = 0\n",
    "    # Iterate through each fold of cross-validation\n",
    "    for n, (train_index, test_index) in enumerate(kf.split(X), start=1):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        lr = LogisticRegression(solver='liblinear')\n",
    "        params = {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1.0, 10.0],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "        gs = GridSearchCV(lr, param_grid=params, cv=5, n_jobs=-1)\n",
    "        gs.fit(X_train, y_train)\n",
    "\n",
    "        rf_best = LogisticRegression(**gs.best_params_, solver='liblinear')\n",
    "        rf_best.fit(X_train, y_train)\n",
    "        # Predict on the test set\n",
    "        y_pred = rf_best.predict(X_test)\n",
    "        y_pred_proba = rf_best.predict_proba(X_test)[:, 1]\n",
    "        TP = np.sum((y_pred == 1) & (y_test == 1))\n",
    "        TN = np.sum((y_pred == 0) & (y_test == 0))\n",
    "        FP = np.sum((y_pred == 1) & (y_test == 0))\n",
    "        FN = np.sum((y_pred == 0) & (y_test == 1))\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        specificity = TN / (TN + FP)\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        MCC = (TP * TN - FP * FN) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "        y_pred_new = rf_best.predict(X_test)\n",
    "        y_pred_proba_new = rf_best.predict_proba(X_test)[:, 1]\n",
    "        y_pred_proba_new_list.append(y_pred_proba_new)\n",
    "        TP_new = np.sum((y_pred_new == 1) & (y_test == 1))\n",
    "        TN_new = np.sum((y_pred_new == 0) & (y_test == 0))\n",
    "        FP_new = np.sum((y_pred_new == 1) & (y_test == 0))\n",
    "        FN_new = np.sum((y_pred_new == 0) & (y_test == 1))\n",
    "        precision_new = TP_new / (TP_new + FP_new)\n",
    "        recall_new = TP_new / (TP_new + FN_new)\n",
    "        specificity_new = TN_new / (TN_new + FP_new)\n",
    "        accuracy_new = (TP_new + TN_new) / (TP_new + TN_new + FP_new + FN_new)\n",
    "        MCC_new = (TP_new * TN_new - FP_new * FN_new) / np.sqrt(\n",
    "            (TP_new + FP_new) * (TP_new + FN_new) * (TN_new + FP_new) * (TN_new + FN_new))\n",
    "        f1_score_new = 2 * (precision_new * recall_new) / (precision_new + recall_new)\n",
    "        fpr_new, tpr_new, _ = roc_curve(y_test, y_pred_proba_new)\n",
    "        roc_auc_new = auc(fpr_new, tpr_new)\n",
    "        precision_curve_new, recall_curve_new, _ = precision_recall_curve(y_test, y_pred_proba_new)\n",
    "        pr_auc_new = auc(recall_curve_new, precision_curve_new)\n",
    "        if best_score < roc_auc_new:\n",
    "            best_score = roc_auc_new\n",
    "            best_n = n\n",
    "            best_model = rf_best\n",
    "        independent_test = pd.DataFrame(y_pred_proba_new, columns=['predictions'])\n",
    "        merge_df = pd.concat([test_data, independent_test], axis=1)\n",
    "        # Record performance metrics\n",
    "        TPs.append(TP_new)\n",
    "        TNs.append(TN_new)\n",
    "        FPs.append(FP_new)\n",
    "        FNs.append(FN_new)\n",
    "        precisions.append(precision_new)\n",
    "        recalls.append(recall_new)\n",
    "        specificities.append(specificity_new)\n",
    "        accuracies.append(accuracy_new)\n",
    "        MCCs.append(MCC_new)\n",
    "        f1_scores.append(f1_score_new)\n",
    "        AUROCs.append(roc_auc_new)\n",
    "        AUPRCs.append(pr_auc_new)\n",
    "        precision_list.append(precision_curve_new)\n",
    "        recall_list.append(recall_curve_new)\n",
    "        pr_auc_list.append(pr_auc_new)\n",
    "        fprs.append(fpr_new)\n",
    "        tprs.append(tpr_new)\n",
    "        aucs.append(roc_auc_new)\n",
    "    col_means = [np.mean(col) for col in zip(*y_pred_proba_new_list)]\n",
    "    mean_independent_test = pd.DataFrame(col_means, columns=['predictions'])\n",
    "    merge_df = pd.concat([test_data, mean_independent_test], axis=1)\n",
    "    if save_model and best_model is not None:\n",
    "        joblib.dump(best_model, f'{encode}.lr.best.pkl')\n",
    "\n",
    "    # Print the performance summary for each fold\n",
    "    print_performance_summary(TPs, TNs, FPs, FNs, precisions, recalls, specificities, accuracies, MCCs, f1_scores, AUROCs, AUPRCs)\n",
    "\n",
    "def print_performance_summary(TPs, TNs, FPs, FNs, precisions, recalls, specificities, accuracies, MCCs, f1_scores, AUROCs, AUPRCs):\n",
    "    metrics = {\n",
    "        'TP': TPs, 'TN': TNs, 'FP': FPs, 'FN': FNs,\n",
    "        'Precision': precisions, 'Recall': recalls, 'Specificity': specificities,\n",
    "        'Accuracy': accuracies, 'MCC': MCCs, 'F1 score': f1_scores,\n",
    "        'AUROC': AUROCs, 'AUPRC': AUPRCs\n",
    "    }\n",
    "    for name, values in metrics.items():\n",
    "        print(f\"{name}: mean = {np.mean(values)}, std = {np.std(values)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_lr(df_protein, test_df_protein, encode=\"Intergration\", save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "import joblib\n",
    "import numpy as np\n",
    "from zzd.utils.assess import multi_scores as scores\n",
    "\n",
    "def load_and_predict(model_path, train_data, test_data, train_file, test_file, output_train, output_test):\n",
    "    \"\"\"\n",
    "    Load the model and make predictions, saving the results to specified files.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_path: Path to the model file\n",
    "    - train_data: DataFrame for training data\n",
    "    - test_data: DataFrame for test data\n",
    "    - train_file: Path to the training set file for generating the prediction table\n",
    "    - test_file: Path to the test set file for generating the prediction table\n",
    "    - output_train: Output filename for training set predictions\n",
    "    - output_test: Output filename for test set predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(model_path, 'rb') as model_file:\n",
    "            loaded_model = joblib.load(model_file)\n",
    "    \n",
    "        model_params = loaded_model.get_params()\n",
    "        print(\"Model hyperparameters:\", model_params)\n",
    "        X_train = train_data.iloc[:, 1:]\n",
    "        y_train = train_data.iloc[:, 0]\n",
    "        X_test = test_data.iloc[:, 1:]\n",
    "        y_test = test_data.iloc[:, 0]\n",
    "\n",
    "        y_train_pred = loaded_model.predict_proba(X_train)[:, 1]\n",
    "        y_test_pred = loaded_model.predict_proba(X_test)[:, 1]\n",
    "        pred_table_train = np.hstack((np.genfromtxt(train_file, str), y_train_pred.reshape(-1, 1)))\n",
    "        pred_table_test = np.hstack((np.genfromtxt(test_file, str), y_test_pred.reshape(-1, 1)))\n",
    "        print(\"Test set evaluation results:\")\n",
    "        result_test = scores(pred_table_test[:, -2], pred_table_test[:, -1], show=True)\n",
    "\n",
    "        # Save prediction results\n",
    "        with open(output_train, 'w') as output_file_train:\n",
    "            for row in pred_table_train:\n",
    "                output_file_train.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "        with open(output_test, 'w') as output_file_test:\n",
    "            for row in pred_table_test:\n",
    "                output_file_test.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "        print(f\"Prediction results have been saved to {output_train} and {output_test}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred during model loading or prediction:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyperparameters: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l1', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "Test set evaluation results:\n",
      "TP\tTN\tFP\tFN\tprecision\trecall\tspecificity\tAcc\tMCC\tf1\tAUROC\tAUPRC\n",
      "121\t114\t37\t28\t0.7658\t0.812\t0.755\t0.783\t0.568\t0.788\t0.851\t0.846\n",
      "Prediction results have been saved to DeepISO.train.txt and DeepISO.test.txt\n"
     ]
    }
   ],
   "source": [
    "train_data = df_protein\n",
    "test_data = test_df_protein\n",
    "model_path = 'DeepISO.LR.pkl'\n",
    "train_file = 'train.txt'  \n",
    "test_file = 'test.txt'   \n",
    "\n",
    "load_and_predict(\n",
    "    model_path=model_path,\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    train_file=train_file,\n",
    "    test_file=test_file,\n",
    "    output_train='DeepISO.train.txt',\n",
    "    output_test='DeepISO.test.txt'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
